import json
import numpy as np
import torch
from transformers import pipeline, AutoTokenizer, TextIteratorStreamer
from threading import Thread
from huggingface_hub import snapshot_download
import os

class InferlessPythonModel:

    # Implement the Load function here for the model
    def initialize(self):
        local_path = "/var/nfs-mount/deepseek"
        model_id = "unsloth/DeepSeek-R1-Distill-Qwen-1.5B-unsloth-bnb-4bit"
        print(os.path.exists(local_path))
        if os.path.exists(local_path) == False:
            os.makedirs(local_path)
            snapshot_download(
                model_id,
                local_dir=local_path
            )
        self.generator = pipeline("text-generation", local_path, torch_dtype=torch.float16, device_map="auto")
        self.tokenizer = AutoTokenizer.from_pretrained(local_path)
        self.streamer = TextIteratorStreamer(self.tokenizer, skip_prompt=True, skip_special_tokens=True)

    # Function to perform inference 
    def infer(self, inputs, stream_output_handler):
        # inputs is a dictonary where the keys are input names and values are actual input data
        # e.g. in the below code the input name is "prompt"
        prompt = inputs["prompt"]
        if prompt == "/check":
            stream_output_handler.send_streamed_output({"OUT": "OK"})
            stream_output_handler.finalise_streamed_output()
            return

        messages = [{ "role": "system", "content": "You are a helpful assistant." }]
        messages.append({ "role": "user", "content": prompt })

        tokenized_chat = self.tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors="pt").cuda()

        generation_kwargs = dict(
            input_ids=tokenized_chat,
            streamer=self.streamer,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            top_k=50,
            max_new_tokens=100,
        )
        def generate():
            model = self.generator.model
            model.generate(**generation_kwargs)
        
        thread = Thread(target=generate)
        thread.start()

        for new_text in self.streamer:
            output_dict = {"OUT": new_text}
            stream_output_handler.send_streamed_output(output_dict)

        thread.join()
        # The output generated by the infer function should be a dictonary where keys are output names and values are actual output data
        # e.g. in the below code the output name is "generated_txt"
        stream_output_handler.finalise_streamed_output()
    # perform any cleanup activity here
    def finalize(self,args):
        self.pipe = None
