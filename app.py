# import json
# import numpy as np
# import torch
# from transformers import pipeline, AutoTokenizer, TextIteratorStreamer
# from threading import Thread
# from huggingface_hub import snapshot_download
# import os

# class InferlessPythonModel:

#     # Implement the Load function here for the model
#     def initialize(self):
#         local_path = "/var/nfs-mount/deepseek"
#         model_id = "unsloth/DeepSeek-R1-Distill-Qwen-1.5B-unsloth-bnb-4bit"
#         print(os.path.exists(local_path))
#         if os.path.exists(local_path) == False:
#             os.makedirs(local_path)
#             snapshot_download(
#                 model_id,
#                 local_dir=local_path
#             )
#         self.generator = pipeline("text-generation", local_path, torch_dtype=torch.float16, device_map="auto")
#         self.tokenizer = AutoTokenizer.from_pretrained(local_path)
#         self.streamer = TextIteratorStreamer(self.tokenizer, skip_prompt=True, skip_special_tokens=True)

#     # Function to perform inference 
#     def infer(self, inputs, stream_output_handler):
#         # inputs is a dictonary where the keys are input names and values are actual input data
#         # e.g. in the below code the input name is "prompt"
#         prompt = inputs["prompt"]
#         if prompt == "/check":
#             stream_output_handler.send_streamed_output({"OUT": "OK"})
#             stream_output_handler.finalise_streamed_output()
#             return

#         messages = [{ "role": "system", "content": "You are a helpful assistant." }]
#         messages.append({ "role": "user", "content": prompt })

#         tokenized_chat = self.tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors="pt").cuda()

#         generation_kwargs = dict(
#             input_ids=tokenized_chat,
#             streamer=self.streamer,
#             do_sample=True,
#             temperature=0.7,
#             top_p=0.9,
#             top_k=50,
#             max_new_tokens=100,
#         )
#         def generate():
#             model = self.generator.model
#             model.generate(**generation_kwargs)
        
#         thread = Thread(target=generate)
#         thread.start()

#         for new_text in self.streamer:
#             output_dict = {"OUT": new_text}
#             stream_output_handler.send_streamed_output(output_dict)

#         thread.join()
#         # The output generated by the infer function should be a dictonary where keys are output names and values are actual output data
#         # e.g. in the below code the output name is "generated_txt"
#         stream_output_handler.finalise_streamed_output()
#     # perform any cleanup activity here
#     def finalize(self,args):
#         self.pipe = None
import json
import numpy as np
import torch
from transformers import pipeline, AutoTokenizer, TextIteratorStreamer, BitsAndBytesConfig
from threading import Thread
from huggingface_hub import snapshot_download
import os

class InferlessPythonModel:
    def initialize(self):
        try:
            local_path = "/var/nfs-mount/deepseek"
            model_id = "unsloth/DeepSeek-R1-Distill-Qwen-1.5B-unsloth-bnb-4bit"
            
            if not os.path.exists(local_path):
                os.makedirs(local_path)
                snapshot_download(
                    model_id,
                    local_dir=local_path,
                    resume_download=True
                )

            # Cấu hình quantization
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
                llm_int8_enable_fp32_cpu_offload=True
            )
            
            # Khởi tạo pipeline với cấu hình mới
            self.generator = pipeline(
                "text-generation",
                local_path,
                torch_dtype=torch.float16,
                device_map="auto",
                quantization_config=quantization_config
            )
            
            self.tokenizer = AutoTokenizer.from_pretrained(local_path)
            self.streamer = TextIteratorStreamer(
                self.tokenizer,
                skip_prompt=True,
                skip_special_tokens=True
            )
            
            # In thông tin về memory usage
            if torch.cuda.is_available():
                print(f"GPU Memory allocated: {torch.cuda.memory_allocated() / 1024**2:.2f}MB")
                print(f"GPU Memory reserved: {torch.cuda.memory_reserved() / 1024**2:.2f}MB")
                
        except Exception as e:
            print(f"Initialization error: {str(e)}")
            raise

    def infer(self, inputs, stream_output_handler):
        try:
            prompt = inputs.get("prompt")
            if not prompt:
                raise ValueError("Prompt is required")

            if prompt == "/check":
                stream_output_handler.send_streamed_output({"OUT": "OK"})
                stream_output_handler.finalise_streamed_output()
                return

            messages = [
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": prompt}
            ]

            input_ids = self.tokenizer.apply_chat_template(
                messages,
                tokenize=True,
                add_generation_prompt=True,
                return_tensors="pt"
            )
            
            # Không cần chuyển input_ids sang device vì model sẽ tự handle
            generation_kwargs = {
                "input_ids": input_ids,
                "do_sample": True,
                "temperature": 0.7,
                "top_p": 0.9,
                "top_k": 50,
                "max_new_tokens": 1024,
                "streamer": self.streamer,
            }

            def generate():
                model = self.generator.model
                model.generate(**generation_kwargs)

            thread = Thread(target=generate)
            thread.start()

            try:
                for new_text in self.streamer:
                    output_dict = {"OUT": new_text}
                    stream_output_handler.send_streamed_output(output_dict)
            except Exception as e:
                print(f"Streaming error: {str(e)}")
                raise
            finally:
                thread.join(timeout=30)

        except Exception as e:
            print(f"Inference error: {str(e)}")
            stream_output_handler.send_streamed_output(
                {"error": f"An error occurred: {str(e)}"}
            )
        finally:
            stream_output_handler.finalise_streamed_output()

    def finalize(self, args):
        try:
            self.generator = None
            self.tokenizer = None
            self.streamer = None
            torch.cuda.empty_cache()
        except Exception as e:
            print(f"Finalization error: {str(e)}")
