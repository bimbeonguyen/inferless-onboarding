import json
import numpy as np
import torch
from transformers import pipeline, AutoTokenizer
from threading import Thread
from huggingface_hub import snapshot_download
import os

class InferlessPythonModel:

    # Implement the Load function here for the model
    def initialize(self):
        local_path = "/var/nfs-mount/deepseek"
        model_id = "unsloth/DeepSeek-R1-Distill-Qwen-1.5B-unsloth-bnb-4bit"
        if os.path.exists(local_path) == False:
            os.makedirs(local_path)
            snapshot_download(
                model_id,
                local_dir=local_path
            )
        self.generator = pipeline("text-generation", local_path, torch_dtype=torch.float16, device_map="auto")
        self.tokenizer = AutoTokenizer.from_pretrained(local_path)
        # self.streamer = TextIteratorStreamer(self.tokenizer, skip_prompt=True, skip_special_tokens=True)

    # Function to perform inference 
    def infer(self, inputs, stream_output_handler):
        # inputs is a dictonary where the keys are input names and values are actual input data
        # e.g. in the below code the input name is "prompt"
        prompt = inputs["prompt"]
        if prompt == "/check":
            return {"generated_text": "OK"}
        pipeline_output = self.generator(prompt, do_sample=True, min_length=20, max_length=100, temperature=0.7, top_p=0.9, top_k=50, num_return_sequences=1)
        generated_txt = pipeline_output[0]["generated_text"]
        # The output generated by the infer function should be a dictonary where keys are output names and values are actual output data
        # e.g. in the below code the output name is "generated_txt"
        return {"generated_text": generated_txt}

    # perform any cleanup activity here
    def finalize(self,args):
        self.pipe = None
