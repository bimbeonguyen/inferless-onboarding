import json
import numpy as np
import torch
from transformers import pipeline


class InferlessPythonModel:

    # Implement the Load function here for the model
    def initialize(self):
        self.generator = pipeline("text-generation", model="unsloth/DeepSeek-R1-Distill-Qwen-1.5B-unsloth-bnb-4bit", torch_dtype=torch.float16, device_map="auto")

    
    # Function to perform inference 
    def infer(self, inputs):
        # inputs is a dictonary where the keys are input names and values are actual input data
        # e.g. in the below code the input name is "prompt"
        prompt = inputs["prompt"]
        if prompt == "/check":
            return {"generated_text": "OK"}
        pipeline_output = self.generator(prompt, do_sample=True, min_length=20, max_length=100, temperature=0.7, top_p=0.9, top_k=50, num_return_sequences=1)
        generated_txt = pipeline_output[0]["generated_text"]
        # The output generated by the infer function should be a dictonary where keys are output names and values are actual output data
        # e.g. in the below code the output name is "generated_txt"
        return {"generated_text": generated_txt}

    def models():
        return {"models": "unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit"}
    # perform any cleanup activity here
    def finalize(self,args):
        self.pipe = None
